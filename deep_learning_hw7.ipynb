{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938b18a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # для работы с данными\n",
    "import time  # для оценки времени\n",
    "import torch  # для написания нейросети\n",
    "from torch import nn\n",
    "import re\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e6feb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd162a",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5867c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция генерации последователльности\n",
    "def get_seq(d=10,n=10000):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(n):\n",
    "        X.append(np.random.randint(0,10,d))\n",
    "        x1 = X[i][0]\n",
    "        y = [ (x + x1) if (x + x1) < 10 else (x + x1 -10) for x in X[i]]\n",
    "        y[0]= x1\n",
    "        Y.append(np.array(y))\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadc345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RK\\AppData\\Local\\Temp\\ipykernel_19016\\3191984709.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  train_dataset = TensorDataset(torch.Tensor(train_data[0]).long(),torch.Tensor(train_data[1]).long())\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 10\n",
    "train_data = get_seq(d=SEQ_LEN,n=100000)\n",
    "train_dataset = TensorDataset(torch.Tensor(train_data[0]).long(),torch.Tensor(train_data[1]).long())\n",
    "train = DataLoader(train_dataset,batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = get_seq(d=SEQ_LEN,n=10000)\n",
    "test_dataset = TensorDataset(torch.Tensor(test_data[0]).long(),torch.Tensor(test_data[1]).long())\n",
    "test = DataLoader(test_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3b5f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 7, 9, 9, 8, 0, 8, 7, 2, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b5aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 6, 8, 8, 7, 9, 7, 6, 1, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a21b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция тренировки n_ex - количество вывводимых результатов из тестовой выборки\n",
    "def train_model(model,n_epoch = 10,nshow=10,n_ex = 10):\n",
    "    for ep in range(n_epoch):\n",
    "        train_iters, train_passed  = 0, 0\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        start=time.time()\n",
    "\n",
    "        model.train()\n",
    "        for X, y in train:\n",
    "            if X.shape[0] !=BATCH_SIZE:\n",
    "                continue\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.view(BATCH_SIZE * seq_len, n_chars)\n",
    "            real_y = y.view(BATCH_SIZE * seq_len).long()\n",
    "            l = loss(y_pred, real_y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += l.item()\n",
    "            train_acc += (y_pred.argmax(dim=1) == real_y).sum().item()/seq_len\n",
    "            train_iters += 1\n",
    "            train_passed += len(X)\n",
    "\n",
    "        \n",
    "        test_iters, test_passed  = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "\n",
    "        model.eval()\n",
    "        for X, y in test:\n",
    "            if X.shape[0]!=BATCH_SIZE:\n",
    "                continue\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.view(BATCH_SIZE * seq_len, n_chars)\n",
    "            real_y = y.view(BATCH_SIZE * seq_len).long()\n",
    "            l = loss(y_pred, real_y)\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred.argmax(dim=1) == real_y).sum().item()/seq_len\n",
    "            test_iters += 1\n",
    "            test_passed += len(X)\n",
    "\n",
    "        if ep%nshow==0:\n",
    "            print(\"ep: {}, taked: {:.3f}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "                ep, time.time() - start, round(train_loss / train_iters,5), round(train_acc / train_passed,5),\n",
    "                round(test_loss / test_iters,5), round(test_acc / test_passed,5))\n",
    "                \n",
    "            )\n",
    "            \n",
    "    return y_pred.argmax(dim=1).view(BATCH_SIZE,-1)[:n_ex],real_y.view(BATCH_SIZE,-1)[:n_ex]\n",
    "\n",
    "\n",
    "#функция для тренировки разных моделей\n",
    "#выводит примеры по 20 символов\n",
    "def m_train(rnn='RNN',lr = 0.01):\n",
    "    model = Network()\n",
    "    if rnn == 'LSTM':\n",
    "        model.rnn = nn.LSTM(embeding_n, hidden_n,batch_first=True)\n",
    "    elif rnn == 'GRU':\n",
    "        model.rnn = nn.GRU(embeding_n, hidden_n,batch_first=True)\n",
    "    else:\n",
    "        if rnn !='RNN':\n",
    "            raise \n",
    "    model.to(device)\n",
    "    global loss \n",
    "    loss = torch.nn.CrossEntropyLoss()  # типичный лосс многоклассовой классификации\n",
    "    global optimizer \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    test_predict,test_real = train_model(model,n_epoch = 10,nshow=1,n_ex = 5)\n",
    "    \n",
    "    print('#####################')\n",
    "    print('test predict')\n",
    "    print('\\n'.join([' '.join(map(str,test_predict[i][:20].to('cpu').numpy())) for i in range(5)]))\n",
    "    print('test predict')\n",
    "    print('\\n'.join([' '.join(map(str,test_real[i][:20].to('cpu').numpy())) for i in range(5)]))\n",
    "    \n",
    "    return test_predict,test_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c273ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = SEQ_LEN\n",
    "hidden_n = 64\n",
    "n_chars = 10\n",
    "embeding_n = 32\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_chars, embeding_n)\n",
    "        self.rnn = torch.nn.RNN(embeding_n, hidden_n,batch_first=True)#num_layers=2\n",
    "        self.out = torch.nn.Linear(hidden_n, n_chars)\n",
    "\n",
    "    def forward(self, sentences, state=None):\n",
    "        x = self.embedding(sentences) \n",
    "        x, s = self.rnn(x) # берём выход с последнего слоя для всех токенов, а не скрытое состояние\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae4dbd",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cccb4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 6.509, train_loss: 0.26054, train_acc: 0.90443, test_loss: 0.15136, test_acc: 0.92481\n",
      "ep: 1, taked: 7.359, train_loss: 0.00903, train_acc: 0.99741, test_loss: 0.00052, test_acc: 1.0\n",
      "ep: 2, taked: 7.847, train_loss: 0.23219, train_acc: 0.90892, test_loss: 0.096, test_acc: 0.96173\n",
      "ep: 3, taked: 7.690, train_loss: 0.08532, train_acc: 0.978, test_loss: 0.00641, test_acc: 0.99959\n",
      "ep: 4, taked: 7.050, train_loss: 0.00212, train_acc: 0.9999, test_loss: 0.0007, test_acc: 0.99997\n",
      "ep: 5, taked: 7.376, train_loss: 0.01325, train_acc: 0.99759, test_loss: 0.39001, test_acc: 0.90636\n",
      "ep: 6, taked: 6.994, train_loss: 0.05893, train_acc: 0.98144, test_loss: 0.00085, test_acc: 1.0\n",
      "ep: 7, taked: 7.454, train_loss: 0.09585, train_acc: 0.98127, test_loss: 0.18978, test_acc: 0.95879\n",
      "ep: 8, taked: 7.386, train_loss: 0.04615, train_acc: 0.99101, test_loss: 0.0042, test_acc: 0.99979\n",
      "ep: 9, taked: 6.870, train_loss: 0.16192, train_acc: 0.9619, test_loss: 0.09636, test_acc: 0.97777\n",
      "#####################\n",
      "test predict\n",
      "7 3 2 1 8 1 9 9 4 6\n",
      "8 3 9 4 9 8 3 2 1 9\n",
      "3 6 4 4 4 6 7 8 0 1\n",
      "3 7 8 4 2 5 6 0 6 6\n",
      "4 1 0 3 5 5 6 8 8 7\n",
      "test predict\n",
      "7 3 2 1 8 1 9 9 4 6\n",
      "8 3 9 4 9 8 3 2 1 9\n",
      "3 6 4 4 4 6 7 8 0 1\n",
      "3 7 8 4 2 5 6 0 6 6\n",
      "4 1 0 3 5 5 6 8 8 7\n"
     ]
    }
   ],
   "source": [
    "predict, real = m_train(rnn='RNN',lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2927b6",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e1c5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 7.915, train_loss: 0.05171, train_acc: 0.983, test_loss: 5e-05, test_acc: 1.0\n",
      "ep: 1, taked: 7.782, train_loss: 2e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 2, taked: 7.959, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 3, taked: 8.004, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 4, taked: 7.872, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 5, taked: 7.972, train_loss: 0.00188, train_acc: 0.99964, test_loss: 3e-05, test_acc: 1.0\n",
      "ep: 6, taked: 7.916, train_loss: 2e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 7, taked: 8.095, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 8, taked: 7.984, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 9, taked: 7.911, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "#####################\n",
      "test predict\n",
      "7 3 2 1 8 1 9 9 4 6\n",
      "8 3 9 4 9 8 3 2 1 9\n",
      "3 6 4 4 4 6 7 8 0 1\n",
      "3 7 8 4 2 5 6 0 6 6\n",
      "4 1 0 3 5 5 6 8 8 7\n",
      "test predict\n",
      "7 3 2 1 8 1 9 9 4 6\n",
      "8 3 9 4 9 8 3 2 1 9\n",
      "3 6 4 4 4 6 7 8 0 1\n",
      "3 7 8 4 2 5 6 0 6 6\n",
      "4 1 0 3 5 5 6 8 8 7\n"
     ]
    }
   ],
   "source": [
    "predict, real = m_train(rnn='LSTM',lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b3c5f",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c4bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 7.758, train_loss: 0.05913, train_acc: 0.98087, test_loss: 0.0001, test_acc: 1.0\n",
      "ep: 1, taked: 7.886, train_loss: 4e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 2, taked: 7.827, train_loss: 1e-05, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 3, taked: 7.865, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 4, taked: 7.848, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 5, taked: 7.813, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 6, taked: 7.875, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 7, taked: 7.840, train_loss: 0.00198, train_acc: 0.99948, test_loss: 3e-05, test_acc: 1.0\n",
      "ep: 8, taked: 7.838, train_loss: 1e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 9, taked: 7.806, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "#####################\n",
      "test predict\n",
      "7 3 2 1 8 1 9 9 4 6\n",
      "8 3 9 4 9 8 3 2 1 9\n",
      "3 6 4 4 4 6 7 8 0 1\n",
      "3 7 8 4 2 5 6 0 6 6\n",
      "4 1 0 3 5 5 6 8 8 7\n",
      "test predict\n",
      "7 3 2 1 8 1 9 9 4 6\n",
      "8 3 9 4 9 8 3 2 1 9\n",
      "3 6 4 4 4 6 7 8 0 1\n",
      "3 7 8 4 2 5 6 0 6 6\n",
      "4 1 0 3 5 5 6 8 8 7\n"
     ]
    }
   ],
   "source": [
    "predict, real = m_train(rnn='GRU',lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4438d",
   "metadata": {},
   "source": [
    "## Увеличим длину последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1690df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "SEQ_LEN = 100\n",
    "train_data = get_seq(d=SEQ_LEN,n=100000)\n",
    "train_dataset = TensorDataset(torch.Tensor(train_data[0]).long(),torch.Tensor(train_data[1]).long())\n",
    "train = DataLoader(train_dataset,batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = get_seq(d=SEQ_LEN,n=10000)\n",
    "test_dataset = TensorDataset(torch.Tensor(test_data[0]).long(),torch.Tensor(test_data[1]).long())\n",
    "test = DataLoader(test_dataset,batch_size=BATCH_SIZE)\n",
    "\n",
    "seq_len = SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a897212",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc03693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 10.007, train_loss: 1.49861, train_acc: 0.28649, test_loss: 1.33623, test_acc: 0.31411\n",
      "ep: 1, taked: 9.176, train_loss: 1.14976, train_acc: 0.38704, test_loss: 1.09756, test_acc: 0.40836\n",
      "ep: 2, taked: 9.390, train_loss: 1.21723, train_acc: 0.4055, test_loss: 1.23173, test_acc: 0.39458\n",
      "ep: 3, taked: 9.330, train_loss: 0.87714, train_acc: 0.49247, test_loss: 0.80032, test_acc: 0.51067\n",
      "ep: 4, taked: 9.351, train_loss: 1.0137, train_acc: 0.46415, test_loss: 1.52717, test_acc: 0.30636\n",
      "ep: 5, taked: 9.295, train_loss: 1.10144, train_acc: 0.42205, test_loss: 0.79715, test_acc: 0.51531\n",
      "ep: 6, taked: 9.303, train_loss: 0.66455, train_acc: 0.63039, test_loss: 0.60956, test_acc: 0.60822\n",
      "ep: 7, taked: 9.371, train_loss: 0.59012, train_acc: 0.64095, test_loss: 0.49523, test_acc: 0.70826\n",
      "ep: 8, taked: 9.394, train_loss: 1.09733, train_acc: 0.449, test_loss: 1.35005, test_acc: 0.34334\n",
      "ep: 9, taked: 9.370, train_loss: 1.07817, train_acc: 0.45406, test_loss: 1.20661, test_acc: 0.41846\n",
      "#####################\n",
      "test predict\n",
      "2 4 3 9 6 9 6 7 1 4 9 6 1 4 7 2 2 3 2 9\n",
      "0 9 7 7 1 5 5 9 3 7 0 7 6 8 3 6 8 7 7 9\n",
      "6 2 4 0 0 1 6 6 1 1 4 8 9 4 3 5 6 1 7 5\n",
      "4 9 7 1 9 9 9 3 7 7 6 6 3 2 4 6 7 1 3 2\n",
      "6 0 6 3 3 2 0 0 2 2 1 2 8 3 4 2 9 1 2 2\n",
      "test predict\n",
      "2 7 2 8 5 8 5 0 4 7 8 5 4 7 6 5 1 3 5 8\n",
      "0 2 6 6 6 6 6 4 3 7 5 0 9 3 6 9 3 2 7 2\n",
      "6 2 4 0 0 1 6 6 1 1 4 8 9 4 3 5 6 1 7 5\n",
      "4 9 8 6 0 0 0 4 8 2 1 1 4 7 5 1 8 6 5 3\n",
      "6 0 6 3 3 2 0 0 2 2 1 2 8 3 4 2 9 1 2 2\n"
     ]
    }
   ],
   "source": [
    "predict, real = m_train(rnn='RNN',lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7abaed1",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f7adaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 9.754, train_loss: 0.38723, train_acc: 0.85242, test_loss: 0.00022, test_acc: 1.0\n",
      "ep: 1, taked: 9.682, train_loss: 0.00141, train_acc: 0.9998, test_loss: 0.00014, test_acc: 1.0\n",
      "ep: 2, taked: 9.682, train_loss: 4e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 3, taked: 9.634, train_loss: 1e-05, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 4, taked: 9.676, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 5, taked: 9.676, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 6, taked: 9.697, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 7, taked: 9.675, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 8, taked: 9.703, train_loss: 0.00081, train_acc: 0.9999, test_loss: 3e-05, test_acc: 1.0\n",
      "ep: 9, taked: 9.680, train_loss: 1e-05, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "#####################\n",
      "test predict\n",
      "2 7 2 8 5 8 5 0 4 7 8 5 4 7 6 5 1 3 5 8\n",
      "0 2 6 6 6 6 6 4 3 7 5 0 9 3 6 9 3 2 7 2\n",
      "6 2 4 0 0 1 6 6 1 1 4 8 9 4 3 5 6 1 7 5\n",
      "4 9 8 6 0 0 0 4 8 2 1 1 4 7 5 1 8 6 5 3\n",
      "6 0 6 3 3 2 0 0 2 2 1 2 8 3 4 2 9 1 2 2\n",
      "test predict\n",
      "2 7 2 8 5 8 5 0 4 7 8 5 4 7 6 5 1 3 5 8\n",
      "0 2 6 6 6 6 6 4 3 7 5 0 9 3 6 9 3 2 7 2\n",
      "6 2 4 0 0 1 6 6 1 1 4 8 9 4 3 5 6 1 7 5\n",
      "4 9 8 6 0 0 0 4 8 2 1 1 4 7 5 1 8 6 5 3\n",
      "6 0 6 3 3 2 0 0 2 2 1 2 8 3 4 2 9 1 2 2\n"
     ]
    }
   ],
   "source": [
    "predict, real = m_train(rnn='LSTM',lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1132bc",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c84fa3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, taked: 9.607, train_loss: 0.16705, train_acc: 0.93602, test_loss: 0.00024, test_acc: 1.0\n",
      "ep: 1, taked: 9.634, train_loss: 0.0001, train_acc: 1.0, test_loss: 3e-05, test_acc: 1.0\n",
      "ep: 2, taked: 9.544, train_loss: 2e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 3, taked: 9.610, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 4, taked: 9.606, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 5, taked: 9.580, train_loss: 0.0, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "ep: 6, taked: 9.675, train_loss: 0.00493, train_acc: 0.99906, test_loss: 0.00055, test_acc: 1.0\n",
      "ep: 7, taked: 9.739, train_loss: 0.00017, train_acc: 1.0, test_loss: 5e-05, test_acc: 1.0\n",
      "ep: 8, taked: 9.674, train_loss: 3e-05, train_acc: 1.0, test_loss: 1e-05, test_acc: 1.0\n",
      "ep: 9, taked: 9.594, train_loss: 1e-05, train_acc: 1.0, test_loss: 0.0, test_acc: 1.0\n",
      "#####################\n",
      "test predict\n",
      "2 7 2 8 5 8 5 0 4 7 8 5 4 7 6 5 1 3 5 8\n",
      "0 2 6 6 6 6 6 4 3 7 5 0 9 3 6 9 3 2 7 2\n",
      "6 2 4 0 0 1 6 6 1 1 4 8 9 4 3 5 6 1 7 5\n",
      "4 9 8 6 0 0 0 4 8 2 1 1 4 7 5 1 8 6 5 3\n",
      "6 0 6 3 3 2 0 0 2 2 1 2 8 3 4 2 9 1 2 2\n",
      "test predict\n",
      "2 7 2 8 5 8 5 0 4 7 8 5 4 7 6 5 1 3 5 8\n",
      "0 2 6 6 6 6 6 4 3 7 5 0 9 3 6 9 3 2 7 2\n",
      "6 2 4 0 0 1 6 6 1 1 4 8 9 4 3 5 6 1 7 5\n",
      "4 9 8 6 0 0 0 4 8 2 1 1 4 7 5 1 8 6 5 3\n",
      "6 0 6 3 3 2 0 0 2 2 1 2 8 3 4 2 9 1 2 2\n"
     ]
    }
   ],
   "source": [
    "predict, real = m_train(rnn='GRU',lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b770e",
   "metadata": {},
   "source": [
    "ВЫВОДЫ: \n",
    "на маленькой длине последовательности все рассмотренные модели справились хорошо, а вот с увеличением длины \n",
    "обычная RNN уже не смогла сделать верные предсказания, LTSM и  GRU справились с этим хорошо. \n",
    "Такое поведения моделей соответствут их архитектуре."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f51d04",
   "metadata": {},
   "source": [
    "# Задание 2 (дополнительное, из лекции)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70cf0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16a9e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 597844\n"
     ]
    }
   ],
   "source": [
    "with open('nietzsche.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('length:', len(text))\n",
    "text = re.sub('[^a-z ]', ' ', text)\n",
    "text = re.sub('\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef126ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_CHAR = sorted(list(set(text)))\n",
    "CHAR_TO_INDEX = {c: i for i, c in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68aca5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sents: 193074\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 40\n",
    "STEP = 3\n",
    "SENTENCES = []\n",
    "NEXT_CHARS = []\n",
    "for i in range(0, len(text) - MAX_LEN, STEP):\n",
    "    SENTENCES.append(text[i: i + MAX_LEN])\n",
    "    NEXT_CHARS.append(text[i + MAX_LEN])\n",
    "print('Num sents:', len(SENTENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ef7b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = torch.zeros((len(SENTENCES), MAX_LEN), dtype=int)\n",
    "Y = torch.zeros((len(SENTENCES)), dtype=int)\n",
    "for i, sentence in enumerate(SENTENCES):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t] = CHAR_TO_INDEX[char]\n",
    "    Y[i] = CHAR_TO_INDEX[NEXT_CHARS[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf3002e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "data = torch.utils.data.DataLoader(dataset, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37b45210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    softmaxed = torch.softmax(preds, 0)\n",
    "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
    "    return probas.argmax()\n",
    "\n",
    "def generate_text():\n",
    "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + MAX_LEN]\n",
    "    generated += sentence\n",
    "\n",
    "    for i in range(MAX_LEN):\n",
    "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
    "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
    "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
    "\n",
    "        preds = model(x_pred.cuda()).cpu()\n",
    "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
    "        generated = generated + next_char\n",
    "\n",
    "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee425f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        _, state = self.hidden(out)\n",
    "        predictions = self.output(state[0].squeeze())\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fe9b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(nn.LSTM, len(CHAR_TO_INDEX), 64, 128, len(CHAR_TO_INDEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "478b5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e94eddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 10.187, Train loss: 1.858\n",
      "re appearing they must still more be clo|gring whad has be gestengr of the form a\n",
      "Epoch 1. Time: 10.434, Train loss: 1.548\n",
      "bundant enjoyment even in one s own suff|eriad of miscents for than philolophyphi\n",
      "Epoch 2. Time: 10.355, Train loss: 1.446\n",
      "an inward self contempt seek to get out |not a displisionance spirately classurar\n",
      "Epoch 3. Time: 10.824, Train loss: 1.386\n",
      "the populace eat and drink and even wher|e worads santing i faith of netherso see\n",
      "Epoch 4. Time: 10.635, Train loss: 1.344\n",
      " against sympathy but to repeat it once |are instance the individual your sensati\n",
      "Epoch 5. Time: 10.667, Train loss: 1.313\n",
      "s must when compared with his own flying| which intoullen sout the love have prew\n",
      "Epoch 6. Time: 10.625, Train loss: 1.289\n",
      "st hesitation for instance the nobleman |in scientificed bust is old being of eff\n",
      "Epoch 7. Time: 10.651, Train loss: 1.269\n",
      "t of his head the contrary of this monst|rowosphere to men you has us mud civiliz\n",
      "Epoch 8. Time: 10.455, Train loss: 1.251\n",
      "e of the inversion of valuations by mean|ing as a thinker a relations of personac\n",
      "Epoch 9. Time: 11.288, Train loss: 1.236\n",
      "ckedness i dislike him why i am not a ma|n a morality his sin like and which is a\n",
      "Epoch 10. Time: 10.498, Train loss: 1.222\n",
      "int aristophanes make one afraid with me|tage fail are decustical distrust is per\n",
      "Epoch 11. Time: 10.236, Train loss: 1.210\n",
      "significance which still obtains a heari|ng in order to an are are taste with the\n",
      "Epoch 12. Time: 10.220, Train loss: 1.199\n",
      "son and to early death punishable not pu|ssed come of sore spannement soul to eng\n",
      "Epoch 13. Time: 10.228, Train loss: 1.189\n",
      "necessarily in individual and exceptiona| pointoration by readily refinement him \n",
      "Epoch 14. Time: 10.525, Train loss: 1.180\n",
      " such abnegation appear hence it passes |any be the different of religious say an\n",
      "Epoch 15. Time: 10.418, Train loss: 1.171\n",
      "at precisely here one should strictly gi|ve and in the increasured and point valu\n",
      "Epoch 16. Time: 10.264, Train loss: 1.163\n",
      "ent which both employ belongs to the dom|ainated and expertence imposity is trarf\n",
      "Epoch 17. Time: 10.242, Train loss: 1.156\n",
      "hich it has finally presented itself to |guscracual synthesing than peepacies and\n",
      "Epoch 18. Time: 10.362, Train loss: 1.149\n",
      "a child left to its own devices this chi|ldren is moreation ideas even there act \n",
      "Epoch 19. Time: 10.528, Train loss: 1.142\n",
      " that it is a source of pain to others b|e wills in the pass at not will enough a\n",
      "Epoch 20. Time: 10.428, Train loss: 1.135\n",
      "le who from sympathy and anxiety for oth|ers aboverewah problem sopision morality\n",
      "Epoch 21. Time: 8.251, Train loss: 1.130\n",
      " only allowed to pass as such a delight |of which a not gaddication and the will \n",
      "Epoch 22. Time: 7.826, Train loss: 1.123\n",
      "hreds into the form of questions from th|eir race feeling of respected may with i\n",
      "Epoch 23. Time: 7.916, Train loss: 1.118\n",
      " an incentive to sins of omission an add|ation that the progress who the knowledg\n",
      "Epoch 24. Time: 7.944, Train loss: 1.114\n",
      "shes peculiar to him who is perhaps hims|elf astory in here that the short discie\n",
      "Epoch 25. Time: 8.058, Train loss: 1.110\n",
      " only be the right disguise for the sham|peansment expresses even his covect perp\n",
      "Epoch 26. Time: 7.701, Train loss: 1.105\n",
      "one he ascribes the success the carrying| of the depression the ears it is dewagr\n",
      "Epoch 27. Time: 7.705, Train loss: 1.100\n",
      "r contemptible and if he is not satisfie|d the ode ideas from childish the bad ma\n",
      "Epoch 28. Time: 7.572, Train loss: 1.096\n",
      "ld and to comprehension thereof this ind|ividual noation suppose of shards and en\n",
      "Epoch 29. Time: 7.622, Train loss: 1.092\n",
      "ence himself suicide in such a case is a| man who would not dominated by the diso\n",
      "Epoch 30. Time: 7.672, Train loss: 1.088\n",
      "ancy as retributive justice occasions hi|gher philosophers no englishment in deep\n",
      "Epoch 31. Time: 7.720, Train loss: 1.085\n",
      "may also be a means of concealing onesel|f ideas the worthy recarms as is already\n",
      "Epoch 32. Time: 7.817, Train loss: 1.082\n",
      " is the universal enduring utility befor|e a sinds that the good really demanded \n",
      "Epoch 33. Time: 7.725, Train loss: 1.078\n",
      "ilosophers themselves even the great chi|ninting itself with a theinged in all an\n",
      "Epoch 34. Time: 7.671, Train loss: 1.076\n",
      "the modern soul everywhere equality befo|re of german his emaceous opposites and \n",
      "Epoch 35. Time: 7.920, Train loss: 1.072\n",
      "se of conduct is pre minently desirable |that he any colding capable into them wo\n",
      "Epoch 36. Time: 7.772, Train loss: 1.070\n",
      "and noble into the tenets of his teacher|ing of them makes so among the spiritual\n",
      "Epoch 37. Time: 7.949, Train loss: 1.067\n",
      "y science in ordinary language on heart |this established and in the outsible dep\n",
      "Epoch 38. Time: 7.684, Train loss: 1.066\n",
      "reaks forth in him it is the slave in th|is eager frightful pity all the galdary \n",
      "Epoch 39. Time: 7.822, Train loss: 1.062\n",
      "e existence of an individual in order to| go which knows from what anxiety or eve\n",
      "Epoch 40. Time: 7.880, Train loss: 1.060\n",
      "n ego as cause and finally of an ego as |much therefore the deservic in the fablo\n",
      "Epoch 41. Time: 7.747, Train loss: 1.059\n",
      " against nature and also against reason |the spirit no longer leavesal could need\n",
      "Epoch 42. Time: 7.694, Train loss: 1.056\n",
      "an is to be made responsible for nothing|s perhaps ons religion is overgoring is \n",
      "Epoch 43. Time: 7.640, Train loss: 1.053\n",
      "ms of skepticism the power to will and t|he earth a loves roalds of such adorate \n",
      "Epoch 44. Time: 7.679, Train loss: 1.052\n",
      "allied sentiments and emotions they stir|l and almost myted at present or first i\n",
      "Epoch 45. Time: 7.788, Train loss: 1.049\n",
      "events the christians of today burning u|pe refined vigole precessation that the \n",
      "Epoch 46. Time: 7.741, Train loss: 1.048\n",
      "e elimination of evil itself which is a |philosopher under of the eyes and full u\n",
      "Epoch 47. Time: 7.790, Train loss: 1.046\n",
      "something which must be surmounted the s|omething conscience is congressible and \n",
      "Epoch 48. Time: 7.637, Train loss: 1.044\n",
      "higher triumph in remaining masters of t|he result what for the right and both ir\n",
      "Epoch 49. Time: 7.583, Train loss: 1.044\n",
      "fluence a human being suddenly collapses| from a god commands and depretence henc\n",
      "Epoch 50. Time: 7.639, Train loss: 1.043\n",
      " mathematics which certainly would never| though have enemneolon and imagine thre\n",
      "Epoch 51. Time: 7.669, Train loss: 1.041\n",
      "t them with good arguments this was the |thing is observe that happiness in the s\n",
      "Epoch 52. Time: 7.593, Train loss: 1.040\n",
      "erial gain to him who would attain knowl|edge of actual where the coumpectitarity\n",
      "Epoch 53. Time: 7.683, Train loss: 1.039\n",
      "s situation had to be increased enormous| his been feelings is eaven jule poets o\n",
      "Epoch 54. Time: 7.910, Train loss: 1.037\n",
      "rits and perhaps ye are also something o|f naposition which is war to befor the l\n",
      "Epoch 55. Time: 8.395, Train loss: 1.036\n",
      "g above all to a resolute faith which do|es also that it into not attempts enemia\n",
      "Epoch 56. Time: 7.661, Train loss: 1.034\n",
      "g seriously it is a fundamental cure for| displained it is carculation estapise l\n",
      "Epoch 57. Time: 7.710, Train loss: 1.035\n",
      " man must open his ears to all the coars|er in his to presence of things aggeans \n",
      "Epoch 58. Time: 7.780, Train loss: 1.033\n",
      "ot belong to a depraved or imperfect and| sopily or else do now into declace of p\n",
      "Epoch 59. Time: 7.754, Train loss: 1.031\n",
      "her to that overplus of plastic healing |the universal deeds dreads affortunation\n",
      "Epoch 60. Time: 7.760, Train loss: 1.033\n",
      "hat god learned greek when he wished to |philosophers in questive influence the s\n",
      "Epoch 61. Time: 7.597, Train loss: 1.029\n",
      "roblem of generation and nutrition it is| tender eighthing times back begins that\n",
      "Epoch 62. Time: 7.577, Train loss: 1.029\n",
      "ent strongest was he by whom such bolt w|ith the last as the as some other men hi\n",
      "Epoch 63. Time: 7.563, Train loss: 1.029\n",
      " are what name for those friends phantom| just the philosopher had the new that w\n",
      "Epoch 64. Time: 7.549, Train loss: 1.029\n",
      "s timidity all the systems of morals whi|ch certain surpasforation who has merely\n",
      "Epoch 65. Time: 7.542, Train loss: 1.029\n",
      " is good to some purpose and as benevole|nce but as a conclusions of human hersel\n",
      "Epoch 66. Time: 7.557, Train loss: 1.029\n",
      "pires to be master and inscribes progres|s infally to the light an honours in its\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67. Time: 7.571, Train loss: 1.025\n",
      "as the peculiarly immoral act so that to| them it is make the general converence \n",
      "Epoch 68. Time: 7.582, Train loss: 1.025\n",
      "ays to replace commanders by the summing| hardly as see for the selition of chris\n",
      "Epoch 69. Time: 7.594, Train loss: 1.025\n",
      "can descend into the nether world of eve|ry surpression which applies printurity \n",
      "Epoch 70. Time: 7.569, Train loss: 1.025\n",
      " dealing with them whereas they all rais|es of europe the man any evolution who i\n",
      "Epoch 71. Time: 7.587, Train loss: 1.025\n",
      "inces courage directness endurance and n|amure mediocre is evalent no tradary wop\n",
      "Epoch 72. Time: 7.549, Train loss: 1.023\n",
      "the free the ever freer spirit to begin |and henum may become himself tead this s\n",
      "Epoch 73. Time: 7.571, Train loss: 1.024\n",
      " same new conditions under which on an a|s others by the theologian to the questi\n",
      "Epoch 74. Time: 7.553, Train loss: 1.024\n",
      "rman depth the only thing necessary for |astumental hent through a thing envy a p\n",
      "Epoch 75. Time: 7.720, Train loss: 1.022\n",
      "nxiety what if it be really imperative t|akes it were the act its own sladdest po\n",
      "Epoch 76. Time: 7.888, Train loss: 1.024\n",
      "clusion as to its author the all suffici|ently constreled of everything is the ge\n",
      "Epoch 77. Time: 7.769, Train loss: 1.023\n",
      " which society has hitherto existed a no|ne of wines which is the functions at my\n",
      "Epoch 78. Time: 7.768, Train loss: 1.022\n",
      "s kept europe for a considerable time un|ity of human appeal of gandative and als\n",
      "Epoch 79. Time: 7.746, Train loss: 1.022\n",
      " little faith in truth bringing with it |were will feel with fihtic could stuped \n",
      "Epoch 80. Time: 7.745, Train loss: 1.023\n",
      "conscience and will have to impose a dec|ourse of sign of hungef owness according\n",
      "Epoch 81. Time: 7.778, Train loss: 1.023\n",
      "losopher has carried his point and that |the expediency to especial course as an \n",
      "Epoch 82. Time: 7.891, Train loss: 1.022\n",
      "ts i think of such men as napoleon goeth|e the conditioned again i have this is i\n",
      "Epoch 83. Time: 8.111, Train loss: 1.021\n",
      "religion gives inducement and opportunit|y man and purefation of birdespers of th\n",
      "Epoch 84. Time: 7.803, Train loss: 1.022\n",
      "ishman have done by means of a stronger |in reality an exchaps at the instinct of\n",
      "Epoch 85. Time: 7.870, Train loss: 1.021\n",
      "d requiring slavery in some form or othe|r valuation of purpose come to be certai\n",
      "Epoch 86. Time: 7.896, Train loss: 1.022\n",
      "d our crime against criminals consists i|n god existence which almost acquinteres\n",
      "Epoch 87. Time: 8.036, Train loss: 1.021\n",
      " the gradations of rank their presumptio|n of the conditions of noble restle logi\n",
      "Epoch 88. Time: 7.936, Train loss: 1.020\n",
      "r of rank of the commonly recognized des|erting for god in those or rich within q\n",
      "Epoch 89. Time: 7.867, Train loss: 1.024\n",
      "far above the human standard in wisdom a|lso in the soul has been order and patac\n",
      "Epoch 90. Time: 7.763, Train loss: 1.026\n",
      "many stages of inner and outer evolution| why involunter feelings of lower years \n",
      "Epoch 91. Time: 7.910, Train loss: 1.023\n",
      "ment between moral conduct and intellect|ual ground dingeral of all what falses b\n",
      "Epoch 92. Time: 7.920, Train loss: 1.021\n",
      "o sacrifice god himself and out of cruel|ty regarded as they way against when the\n",
      "Epoch 93. Time: 7.891, Train loss: 1.022\n",
      "ogous in the case of the unjust judge an|d uncoveseni negard to me rome to be and\n",
      "Epoch 94. Time: 8.106, Train loss: 1.024\n",
      " as may be gained from schopenhauer s re|purectims of birdistic spict is at all n\n",
      "Epoch 95. Time: 8.009, Train loss: 1.020\n",
      " with reference to pleasure and pain bet|ter changes on let this flief knde for g\n",
      "Epoch 96. Time: 8.019, Train loss: 1.023\n",
      "ong us beyond the middle class world and| has pride and feel for some exaltaning \n",
      "Epoch 97. Time: 8.170, Train loss: 1.022\n",
      "es of delusions of the reason that it wa|s predictions on one has to drightful en\n",
      "Epoch 98. Time: 7.911, Train loss: 1.020\n",
      "ivine and determine what sort of history| of a thing powerfully an addents under \n",
      "Epoch 99. Time: 7.847, Train loss: 1.024\n",
      "e discovery of which other ages may envy|sy buder of the welfare in game of music\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "for ep in range(100):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    model.train()\n",
    "    for X_b, y_b in data:\n",
    "        X_b, y_b = X_b.cuda(), y_b.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        answers = model(X_b)\n",
    "        loss = criterion(answers, y_b)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
    "    model.eval()\n",
    "    generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
